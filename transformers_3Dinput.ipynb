{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b06d23-47f0-4bb6-a80f-d84aaf332341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True #si puo' cancellare, Ã¨ per avere l'autocompletion su jupyter notebook ma non pare funzionare\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from utils import *\n",
    "\n",
    "class PointsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, padding=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing subfolders, each representing a sample.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.sim_info = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_dir, \"Simulations_Info.csv\"), newline='') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    index = int(row[\"INDEX\"])\n",
    "                    completed = row[\"COMPLETED\"].strip().lower() in [\"true\", \"1\", \"yes\"]\n",
    "                    self.sim_info.append((index, completed))\n",
    "        except:\n",
    "            self.sim_info = [(i, True) for i in range(5001)]\n",
    "\n",
    "        # Iterate over each subfolder in root_dir and load all data\n",
    "        for subfolder in os.listdir(root_dir):\n",
    "            subfolder_path = os.path.join(root_dir, subfolder)\n",
    "            sample_idx = [int(s) for s in re.findall(r'\\d+', subfolder)]  # Extract the first number in the subfolder name\n",
    "            if sample_idx:\n",
    "                sample_idx = sample_idx[0]\n",
    "            else:\n",
    "                continue\n",
    "           \n",
    "            completed = next((item[1] for item in self.sim_info if item[0] == sample_idx), None) #completed refers to abaqus simulation stsatus\n",
    "            if os.path.isdir(subfolder_path) and sample_idx and completed:\n",
    "                init_coords_circle_file = next((f for f in os.listdir(subfolder_path) if f.endswith('input_coordinates_circle_1.csv')), None)\n",
    "                before_coords_file = next((f for f in os.listdir(subfolder_path) if f.endswith('input_coordinates_circle_2.csv')), None)\n",
    "                gt_file = next((f for f in os.listdir(subfolder_path) if f.endswith('output_displacement_external.csv')), None)\n",
    "                init_coord_plate_file = os.path.join(root_dir,'plate_initial_coordinates.csv')\n",
    "                # Load ground truth data (displacement external)\n",
    "                init_coord_plate_data = pd.read_csv(os.path.join(subfolder_path, init_coord_plate_file), skiprows=0, usecols=[1, 2, 3])\n",
    "                init_coord_plate_data = init_coord_plate_data.to_numpy().flatten()\n",
    "                init_coord_plate_data = torch.tensor(init_coord_plate_data).float()\n",
    "                \n",
    "                if init_coords_circle_file and before_coords_file and gt_file and init_coord_plate_file:\n",
    "\n",
    "                    #load initial circle data\n",
    "                    init_coords_circle_data = pd.read_csv(os.path.join(subfolder_path, init_coords_circle_file), skiprows=0, usecols=[1, 2, 3])\n",
    "                    init_coords_circle_data = init_coords_circle_data.to_numpy()\n",
    "                    init_coords_circle_data = torch.tensor(init_coords_circle_data).float()\n",
    "\n",
    "                    #load before_impact circle data\n",
    "                    before_coords_data = pd.read_csv(os.path.join(subfolder_path, before_coords_file), skiprows=0, usecols=[1, 2, 3])\n",
    "                    before_coords_data = before_coords_data.to_numpy()\n",
    "                    before_coords_data = torch.tensor(before_coords_data).float()\n",
    "                    \n",
    "                    total_data = torch.cat((init_coords_circle_data,before_coords_data),1)\n",
    "                    if (padding is not None) and total_data.shape[0]<padding:\n",
    "                        total_data = zero_pad_tensor(total_data,target_size=padding)\n",
    "                    #import pdb;pdb.set_trace()\n",
    "                    gt_data = pd.read_csv(os.path.join(subfolder_path, gt_file), skiprows=0, usecols=[1, 2, 3])\n",
    "                    gt_data = gt_data.to_numpy().flatten()\n",
    "                    gt_data = torch.tensor(gt_data).float()\n",
    "\n",
    "                    # Append the final_image, gt_data, init_coord_plate_data tuple to the data list\n",
    "                    self.data.append((total_data, gt_data, init_coord_plate_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve preloaded data\n",
    "        total_data, gt_data, init_coord_plate_data = self.data[idx]\n",
    "\n",
    "        # Apply transformation if available\n",
    "        if self.transform:\n",
    "            total_data = self.transform(total_data)\n",
    "\n",
    "        return total_data, gt_data, init_coord_plate_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481061d-5a98-4cc3-8471-1d00b03623b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 461822839\n",
      "--Return--\n",
      "None\n",
      "> \u001b[32m/tmp/ipykernel_11565/4200543878.py\u001b[39m(\u001b[92m186\u001b[39m)\u001b[36m<module>\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    184\u001b[39m test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[32m    185\u001b[39m norm_values=calculate_target_normalization(train_loader)\n",
      "\u001b[32m--> 186\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m    187\u001b[39m learning_rate = \u001b[32m0.0001\u001b[39m\n",
      "\u001b[32m    188\u001b[39m n_epochs = \u001b[32m1000\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  norm_values.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** AttributeError: 'dict' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  norm_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': array([0., 0., 0., ..., 0., 0., 0.]), 'std': array([0.0001, 0.0001, 0.0001, ..., 0.0001, 0.0001, 0.0001])}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  norm_values['mean']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 0., 0., ..., 0., 0., 0.])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  norm_values['mean'].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9078,)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import  ResNet18_Weights\n",
    "import copy\n",
    "from transformers import ConvNextConfig, ConvNextModel\n",
    "import math \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from utils import *\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]  # Add positional encoding\n",
    "        return x\n",
    "\n",
    "class Transformer2DPointsModel(nn.Module):\n",
    "    def __init__(self, input_dim,input_seq_len, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, dropout=0.1):\n",
    "        super(Transformer2DPointsModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear_in = nn.Linear(input_dim, d_model)  # Project input to d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True),\n",
    "            num_encoder_layers\n",
    "        )\n",
    "        self.attention = AttentionLayer(d_model)  # Add attention layer\n",
    "        #self.linear_out = nn.Linear(d_model, output_dim)  # Map to the desired output size\n",
    "        self.linear_out = nn.Linear(d_model*input_seq_len, output_dim)  # Map to the desired output size\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.linear_in(src)  # Project input to d_model\n",
    "        src = self.positional_encoding(src)  # Add positional encoding\n",
    "        #memory = self.transformer_encoder(src, src_key_padding_mask=src_mask)  # Pass through transformer encoder\n",
    "                # memory shape: (batch_size, seq_len, d_model)\n",
    "        # Zero out padded tokens using src_mask\n",
    "        if src_mask is not None:\n",
    "            memory = memory * src_mask.unsqueeze(-1)\n",
    "            src_key_padding_mask = (src_mask == 0) # Convert 0/1 mask to boolean\n",
    "            memory = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        else:\n",
    "            memory = self.transformer_encoder(src)\n",
    "            \n",
    "        # Apply attention mechanism\n",
    "        #aggregated = self.attention(memory, src_mask)  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Map to the desired output size\n",
    "        #output = self.linear_out(aggregated)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        memory = memory.view(memory.shape[0], -1)  # Shape: (batch_size, d_model, seq_len)\n",
    "        # Map to the desired output size\n",
    "        output = self.linear_out(memory)  # Shape: (batch_size, output_dim)\n",
    "        return output\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=288, out_features=2493, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=2493, out_features=1386, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=1386, out_features=280, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = src.view(src.shape[0],-1)\n",
    "        x = self.fc1(self.relu(src))\n",
    "        x = self.fc2(self.relu(x))\n",
    "        x = self.fc3(self.relu(x))\n",
    "        return x\n",
    "\n",
    "        \n",
    "# Training Setup with Best Model Saving\n",
    "def train_model(model, epochs=3, learning_rate=0.0001, optimizer= None, scheduler = None, train_loader=None, test_loader=None, norm_values=None):\n",
    "    #smoothness_loss_fn = SmoothnessLoss(weight=0.0)  # starting weight\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize variables for tracking best model\n",
    "    best_model_weights = None\n",
    "    best_test_loss = float('inf')\n",
    "    best_epoch=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        #smoothness_loss_fn.weight = time_varying_weight(epoch, 10, 20, max_weight=0.1)\n",
    "        for images, targets, init_data in train_loader:\n",
    "            images = images.to(device)\n",
    "            attention_mask = create_attention_mask(images).to(device)\n",
    "            targets = normalize_targets(targets,norm_values).to(device)              \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images,None)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            #loss2 = smoothness_loss_fn(outputs)\n",
    "            loss = loss #+ loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()  # Update learning rate\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                denorm_outputs = denormalize_targets(outputs, norm_values)\n",
    "                denorm_targets = denormalize_targets(targets,norm_values)\n",
    "                denorm_loss = criterion(denorm_outputs, denorm_targets)\n",
    "                running_loss += denorm_loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss = running_loss/len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.5f}\",end='')\n",
    "        writer.add_scalar('Loss/train', train_loss*1000, epoch+1)\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, targets, init_data in test_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device) \n",
    "                attention_mask = create_attention_mask(images).to(device)\n",
    "                outputs = model(images,None)\n",
    "                loss = criterion(denormalize_targets(outputs,norm_values), targets)\n",
    "                test_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        mean_test_loss = test_loss/len(test_loader.dataset)\n",
    "        print(f\", Test Loss: {mean_test_loss:.5f}\")\n",
    "        writer.add_scalar('Loss/test', mean_test_loss*1000, epoch+1)\n",
    "        # Save the model weights if this epoch has the lowest test loss so far\n",
    "        if mean_test_loss < best_test_loss:\n",
    "            torch.save(model, \"./full_model.pth\")\n",
    "            torch.save(model.state_dict(), \"model_state_dict.pth\")\n",
    "            best_test_loss = mean_test_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            print(f\"New best model saved with test loss: {best_test_loss:.5f}\")\n",
    "            best_epoch = epoch\n",
    "            writer.add_scalar('Loss/best_test', best_test_loss*1000, best_epoch+1)\n",
    "        writer.flush()\n",
    "    # Load the best model weights at the end of training\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"Loaded best model weights with test loss: {best_test_loss:.5f}, epoch: {best_epoch+1:3d}\")\n",
    "    else:\n",
    "        print(\"Warning: No best model weights found.\")\n",
    "    \n",
    "    print(\"Training Complete\")\n",
    "    return model\n",
    "\n",
    "writer = SummaryWriter('runs/experiment_name',flush_secs=5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_seq_len = 98\n",
    "input_dim = 6  # Each input is a sequence of 3Dx2 points (x, y, z)\n",
    "d_model = 512  # Embedding dimension\n",
    "nhead = 4  # Number of attention heads\n",
    "num_encoder_layers = 4  # Number of transformer encoder layers\n",
    "dim_feedforward = 512  # Feedforward network dimension\n",
    "output_dim = 9078  # Each output is a 3D point (x, y, z) and we have 3026 points.\n",
    "dropout = 0.0\n",
    "\n",
    "model = Transformer2DPointsModel(input_dim, input_seq_len, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, dropout).to(device)\n",
    "#model = MLP().to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "#dataset = ImagePairDataset(\"/mnt/hdd1/deformazione-2d-parametrica/\", transform=transform)\n",
    "dataset = PointsDataset(\"/home/fabiana/libraries/my_work/simulazioni_3d/\", transform=None,padding=input_seq_len)\n",
    "test_split = 0.2 \n",
    "batch_size = 128\n",
    "# Split dataset into train and test sets\n",
    "test_size = int(test_split * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "norm_values=calculate_target_normalization(train_loader)\n",
    "import pdb; pdb.set_trace()\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 1000\n",
    "warmup_steps = 100  # Number of warm-up steps\n",
    "total_steps = n_epochs * len(train_loader)  # Total training steps\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps, pct_start=warmup_steps/total_steps)\n",
    "\n",
    "trained_model = train_model(model=model, epochs=n_epochs,learning_rate=learning_rate, optimizer=optimizer, scheduler=scheduler, train_loader=train_loader,test_loader=test_loader,norm_values=norm_values)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f1584-53d7-4b60-accd-ae7faf745d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccdd39-f7ac-4071-9d14-d0b0d732797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def plot_points_sequence_3d_separate(init_coord_data, gt_data, predicted_displacements, connect_points=True):\n",
    "    \"\"\"\n",
    "    Plots two separate 3D figures: one for ground truth points and one for predicted points.\n",
    "\n",
    "    Args:\n",
    "        init_coord_data (numpy.ndarray): Initial positions of shape (N, 3).\n",
    "        gt_data (numpy.ndarray): Ground truth displacements of shape (N, 3).\n",
    "        predicted_displacements (numpy.ndarray): Predicted displacements of shape (N, 3).\n",
    "        connect_points (bool): Whether to draw lines connecting successive points.\n",
    "    \"\"\"\n",
    "    # Compute final positions\n",
    "    gt_points = init_coord_data + gt_data\n",
    "    predicted_points = init_coord_data + predicted_displacements\n",
    "\n",
    "    # ---------- Ground Truth Figure ----------\n",
    "    fig_gt = plt.figure(figsize=(10, 8))\n",
    "    ax_gt = fig_gt.add_subplot(111, projection='3d')\n",
    "    ax_gt.scatter(gt_points[:, 0], gt_points[:, 1], gt_points[:, 2],\n",
    "                  color='green', label='Ground Truth Points', marker='x', s=10)\n",
    "\n",
    "    if connect_points:\n",
    "        for i in range(len(gt_points) - 1):\n",
    "            ax_gt.plot([gt_points[i, 0], gt_points[i + 1, 0]],\n",
    "                       [gt_points[i, 1], gt_points[i + 1, 1]],\n",
    "                       [gt_points[i, 2], gt_points[i + 1, 2]], 'g-', alpha=0.5)\n",
    "\n",
    "    ax_gt.set_title('Ground Truth 3D Points')\n",
    "    ax_gt.set_xlabel('X')\n",
    "    ax_gt.set_ylabel('Y')\n",
    "    ax_gt.set_zlabel('Z')\n",
    "    ax_gt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # ---------- Predicted Points Figure ----------\n",
    "    fig_pred = plt.figure(figsize=(10, 8))\n",
    "    ax_pred = fig_pred.add_subplot(111, projection='3d')\n",
    "    ax_pred.scatter(predicted_points[:, 0], predicted_points[:, 1], predicted_points[:, 2],\n",
    "                    color='red', label='Predicted Points', marker='o', s=10)\n",
    "\n",
    "    if connect_points:\n",
    "        for i in range(len(predicted_points) - 1):\n",
    "            ax_pred.plot([predicted_points[i, 0], predicted_points[i + 1, 0]],\n",
    "                         [predicted_points[i, 1], predicted_points[i + 1, 1]],\n",
    "                         [predicted_points[i, 2], predicted_points[i + 1, 2]], 'r-', alpha=0.5)\n",
    "\n",
    "    ax_pred.set_title('Predicted 3D Points')\n",
    "    ax_pred.set_xlabel('X')\n",
    "    ax_pred.set_ylabel('Y')\n",
    "    ax_pred.set_zlabel('Z')\n",
    "    ax_pred.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show both figures\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for a given sample index\n",
    "# Assuming `init_coord_data`, `gt_data`, and `predicted_displacements` are NumPy arrays of shape (N, 2)\n",
    "# For example:\n",
    "images, targets, init_data = test_loader.dataset[78]\n",
    "images = images.to(device)\n",
    "images = images[None, :, :]\n",
    "attention_mask = create_attention_mask(images).to(device)\n",
    "init_data = init_data.numpy().reshape(-1,3)\n",
    "targets = targets.numpy().reshape(-1,3)\n",
    "predicted_displacements = trained_model(images,None)\n",
    "predicted_displacements = denormalize_targets(predicted_displacements,norm_values)\n",
    "predicted_displacements = predicted_displacements.cpu().detach().numpy()\n",
    "predicted_displacements = predicted_displacements.reshape(-1,3)\n",
    "#import pdb; pdb.set_trace()\n",
    "# Call the function to plot the points for a sample\n",
    "plot_points_sequence_3d_separate(init_data, targets, predicted_displacements,connect_points=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c2669-c25d-4af2-8dea-ec18cebdccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tensor(image_tensor):\n",
    "    \"\"\"\n",
    "    Takes a PyTorch tensor representing image data and plots the image.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A tensor representing image data with shape \n",
    "                                     (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    # Check if the tensor is a single image or a batch\n",
    "    if image_tensor.ndimension() == 4:\n",
    "        # If batch size is greater than 1, let's plot the first image\n",
    "        image_tensor = image_tensor[0]  # Take the first image in the batch\n",
    "    \n",
    "    # Convert the tensor to a NumPy array and move channels to last dimension (for plotting)\n",
    "    image_array = image_tensor.numpy().transpose((1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    #image_array[:, :, 1] = 1\n",
    "    # If the image is grayscale (single channel), it will have only one channel\n",
    "    if image_array.shape[2] == 1:\n",
    "        image_array = image_array[:, :, 0]  # Convert to 2D for grayscale images\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "plot_image_tensor(images[:,:,:,:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9831b-c652-4fa9-aa30-42f7761b3648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6540c-dabf-4ba1-8fd1-8f90123a4437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59667800-296c-4c82-b5b4-b862b54cb38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f820c5-d50a-4717-9156-dd2713dfeca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c593dc-8b84-420c-807c-00dcc6475145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecb627-50ba-4902-98d9-8ba77833c464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c59825-2bea-4bce-bafd-daa030968ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105650f6-4a22-4613-8195-45a38920e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f373f-1019-41a2-b044-6127d0d91f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca485dca-55f0-446e-aa0e-e67e73e09f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ee35a-9256-4ad3-b3cf-a3b2b7c46af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
