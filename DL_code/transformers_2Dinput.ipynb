{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95b06d23-47f0-4bb6-a80f-d84aaf332341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True #si puo' cancellare, Ã¨ per avere l'autocompletion su jupyter notebook ma non pare funzionare\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from utils import *\n",
    "\n",
    "class PointsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, padding=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing subfolders, each representing a sample.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.sim_info = []\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(self.root_dir, \"Simulations_Info.csv\"), newline='') as file:\n",
    "                reader = csv.DictReader(file)\n",
    "                for row in reader:\n",
    "                    index = int(row[\"INDEX\"])\n",
    "                    completed = row[\"COMPLETED\"].strip().lower() in [\"true\", \"1\", \"yes\"]\n",
    "                    self.sim_info.append((index, completed))\n",
    "        except:\n",
    "            self.sim_info = [(i, True) for i in range(5001)]\n",
    "\n",
    "        # Iterate over each subfolder in root_dir and load all data\n",
    "        for subfolder in os.listdir(root_dir):\n",
    "            subfolder_path = os.path.join(root_dir, subfolder)\n",
    "            sample_idx = [int(s) for s in re.findall(r'\\d+', subfolder)]  # Extract the first number in the subfolder name\n",
    "            if sample_idx:\n",
    "                sample_idx = sample_idx[0]\n",
    "            else:\n",
    "                continue\n",
    "           \n",
    "            completed = next((item[1] for item in self.sim_info if item[0] == sample_idx), None) #completed refers to abaqus simulation stsatus\n",
    "            if os.path.isdir(subfolder_path) and sample_idx and completed:\n",
    "                init_coords_circle_file = next((f for f in os.listdir(subfolder_path) if f.endswith('initial_coordinates_circle.csv')), None)\n",
    "                before_coords_file = next((f for f in os.listdir(subfolder_path) if f.endswith('before_impact_coordinates_circle.csv')), None)\n",
    "                gt_file = next((f for f in os.listdir(subfolder_path) if f.endswith('output_displacement_external.csv')), None)\n",
    "                init_coord_plate_file = next((f for f in os.listdir(subfolder_path) if f.endswith('initial_coordinates_plate.csv')), None)\n",
    "                #\n",
    "                if init_coords_circle_file and before_coords_file and gt_file and init_coord_plate_file:\n",
    "\n",
    "                    #load initial circle data\n",
    "                    init_coords_circle_data = pd.read_csv(os.path.join(subfolder_path, init_coords_circle_file), skiprows=0, usecols=[1, 2])\n",
    "                    init_coords_circle_data = init_coords_circle_data.to_numpy()\n",
    "                    init_coords_circle_data = torch.tensor(init_coords_circle_data).float()\n",
    "\n",
    "                    #load before_impact circle data\n",
    "                    before_coords_data = pd.read_csv(os.path.join(subfolder_path, before_coords_file), skiprows=0, usecols=[1, 2])\n",
    "                    before_coords_data = before_coords_data.to_numpy()\n",
    "                    before_coords_data = torch.tensor(before_coords_data).float()\n",
    "                    \n",
    "                    # Load ground truth data (displacement external)\n",
    "                    init_coord_plate_data = pd.read_csv(os.path.join(subfolder_path, init_coord_plate_file), skiprows=0, usecols=[1, 2])\n",
    "                    init_coord_plate_data = init_coord_plate_data.to_numpy().flatten()\n",
    "                    init_coord_plate_data = torch.tensor(init_coord_plate_data).float()\n",
    "\n",
    "                    total_data = torch.cat((init_coords_circle_data,before_coords_data),1)\n",
    "                    if padding is not None:\n",
    "                        total_data = zero_pad_tensor(total_data,target_size=padding)\n",
    "                    #import pdb;pdb.set_trace()\n",
    "                    gt_data = pd.read_csv(os.path.join(subfolder_path, gt_file), skiprows=0, usecols=[1, 2])\n",
    "                    gt_data = gt_data.to_numpy().flatten()\n",
    "                    gt_data = torch.tensor(gt_data).float()\n",
    "\n",
    "                    # Append the final_image, gt_data, init_coord_plate_data tuple to the data list\n",
    "                    self.data.append((total_data, gt_data, init_coord_plate_data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve preloaded data\n",
    "        total_data, gt_data, init_coord_plate_data = self.data[idx]\n",
    "\n",
    "        # Apply transformation if available\n",
    "        if self.transform:\n",
    "            total_data = self.transform(total_data)\n",
    "\n",
    "        return total_data, gt_data, init_coord_plate_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6481061d-5a98-4cc3-8471-1d00b03623b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 16637209\n",
      "Epoch [1/600], Train Loss: 2.2790, Test Loss: 1.0412\n",
      "New best model saved with test loss: 1.0412\n",
      "Epoch [2/600], Train Loss: 0.7772, Test Loss: 0.4252\n",
      "New best model saved with test loss: 0.4252\n",
      "Epoch [3/600], Train Loss: 0.7645, Test Loss: 0.5722\n",
      "Epoch [4/600], Train Loss: 0.5297, Test Loss: 1.8043\n",
      "Epoch [5/600], Train Loss: 1.3538, Test Loss: 0.3936\n",
      "New best model saved with test loss: 0.3936\n",
      "Epoch [6/600], Train Loss: 0.3956, Test Loss: 0.4466\n",
      "Epoch [7/600], Train Loss: 0.3079, Test Loss: 0.2833\n",
      "New best model saved with test loss: 0.2833\n",
      "Epoch [8/600], Train Loss: 0.3292, Test Loss: 0.1958\n",
      "New best model saved with test loss: 0.1958\n",
      "Epoch [9/600], Train Loss: 0.2342, Test Loss: 0.2547\n",
      "Epoch [10/600], Train Loss: 0.1918, Test Loss: 0.3917\n",
      "Epoch [11/600], Train Loss: 0.2486, Test Loss: 0.1901\n",
      "New best model saved with test loss: 0.1901\n",
      "Epoch [12/600], Train Loss: 0.1249, Test Loss: 0.2441\n",
      "Epoch [13/600], Train Loss: 0.2220, Test Loss: 0.1699\n",
      "New best model saved with test loss: 0.1699\n",
      "Epoch [14/600], Train Loss: 0.1325, Test Loss: 0.1776\n",
      "Epoch [15/600], Train Loss: 0.1511, Test Loss: 0.2218\n",
      "Epoch [16/600], Train Loss: 0.1444, Test Loss: 0.3393\n",
      "Epoch [17/600], Train Loss: 0.1679, Test Loss: 0.0968\n",
      "New best model saved with test loss: 0.0968\n",
      "Epoch [18/600], Train Loss: 0.1198, Test Loss: 0.4168\n",
      "Epoch [19/600], Train Loss: 0.1513, Test Loss: 0.1157\n",
      "Epoch [20/600], Train Loss: 0.1025, Test Loss: 0.1697\n",
      "Epoch [21/600], Train Loss: 0.1156, Test Loss: 0.0957\n",
      "New best model saved with test loss: 0.0957\n",
      "Epoch [22/600], Train Loss: 0.0989, Test Loss: 0.1260\n",
      "Epoch [23/600], Train Loss: 0.0878, Test Loss: 0.1454\n",
      "Epoch [24/600], Train Loss: 0.1362, Test Loss: 0.1996\n",
      "Epoch [25/600], Train Loss: 0.1063, Test Loss: 0.1938\n",
      "Epoch [26/600], Train Loss: 0.1780, Test Loss: 0.1335\n",
      "Epoch [27/600], Train Loss: 0.2086, Test Loss: 0.1052\n",
      "Epoch [28/600], Train Loss: 0.1294, Test Loss: 0.1953\n",
      "Epoch [29/600], Train Loss: 0.0908, Test Loss: 0.1778\n",
      "Epoch [30/600], Train Loss: 0.0967, Test Loss: 0.0973\n",
      "Epoch [31/600], Train Loss: 0.0565, Test Loss: 0.0824\n",
      "New best model saved with test loss: 0.0824\n",
      "Epoch [32/600], Train Loss: 0.0572, Test Loss: 0.0483\n",
      "New best model saved with test loss: 0.0483\n",
      "Epoch [33/600], Train Loss: 0.0901, Test Loss: 0.0929\n",
      "Epoch [34/600], Train Loss: 0.0992, Test Loss: 0.0860\n",
      "Epoch [35/600], Train Loss: 0.0697, Test Loss: 0.0775\n",
      "Epoch [36/600], Train Loss: 0.0593, Test Loss: 0.0541\n",
      "Epoch [37/600], Train Loss: 0.0565, Test Loss: 0.0510\n",
      "Epoch [38/600], Train Loss: 0.0510, Test Loss: 0.1022\n",
      "Epoch [39/600], Train Loss: 0.0794, Test Loss: 0.0518\n",
      "Epoch [40/600], Train Loss: 0.1022, Test Loss: 0.0948\n",
      "Epoch [41/600], Train Loss: 0.0521, Test Loss: 0.0899\n",
      "Epoch [42/600], Train Loss: 0.0547, Test Loss: 1.0499\n",
      "Epoch [43/600], Train Loss: 0.5214, Test Loss: 0.2368\n",
      "Epoch [44/600], Train Loss: 0.1898, Test Loss: 0.1695\n",
      "Epoch [45/600], Train Loss: 0.1770, Test Loss: 0.1492\n",
      "Epoch [46/600], Train Loss: 0.1243, Test Loss: 0.0834\n",
      "Epoch [47/600], Train Loss: 0.1515, Test Loss: 0.1394\n",
      "Epoch [48/600], Train Loss: 0.1300, Test Loss: 0.1367\n",
      "Epoch [49/600], Train Loss: 0.1012, Test Loss: 0.5649\n",
      "Epoch [50/600], Train Loss: 0.3089, Test Loss: 0.1677\n",
      "Epoch [51/600], Train Loss: 0.1142, Test Loss: 0.1719\n",
      "Epoch [52/600], Train Loss: 0.1433, Test Loss: 0.1276\n",
      "Epoch [53/600], Train Loss: 0.0654, Test Loss: 0.0976\n",
      "Epoch [54/600], Train Loss: 0.0635, Test Loss: 0.0947\n",
      "Epoch [55/600], Train Loss: 0.0689, Test Loss: 0.0928\n",
      "Epoch [56/600], Train Loss: 0.0691, Test Loss: 0.1685\n",
      "Epoch [57/600], Train Loss: 0.0800, Test Loss: 0.1169\n",
      "Epoch [58/600], Train Loss: 0.0784, Test Loss: 0.1077\n",
      "Epoch [59/600], Train Loss: 0.0492, Test Loss: 0.1254\n",
      "Epoch [60/600], Train Loss: 0.0390, Test Loss: 0.0825\n",
      "Epoch [61/600], Train Loss: 0.1222, Test Loss: 0.1001\n",
      "Epoch [62/600], Train Loss: 0.0617, Test Loss: 0.0601\n",
      "Epoch [63/600], Train Loss: 0.1026, Test Loss: 0.1206\n",
      "Epoch [64/600], Train Loss: 0.0501, Test Loss: 0.0888\n",
      "Epoch [65/600], Train Loss: 0.0602, Test Loss: 0.0514\n",
      "Epoch [66/600], Train Loss: 0.0407, Test Loss: 0.0578\n",
      "Epoch [67/600], Train Loss: 0.1281, Test Loss: 0.1552\n",
      "Epoch [68/600], Train Loss: 0.0626, Test Loss: 0.0743\n",
      "Epoch [69/600], Train Loss: 0.0389, Test Loss: 0.0460\n",
      "New best model saved with test loss: 0.0460\n",
      "Epoch [70/600], Train Loss: 0.0492, Test Loss: 0.0734\n",
      "Epoch [71/600], Train Loss: 0.0455, Test Loss: 0.1047\n",
      "Epoch [72/600], Train Loss: 0.0413, Test Loss: 0.0590\n",
      "Epoch [73/600], Train Loss: 0.0872, Test Loss: 0.0703\n",
      "Epoch [74/600], Train Loss: 0.0949, Test Loss: 0.1952\n",
      "Epoch [75/600], Train Loss: 0.0933, Test Loss: 0.0690\n",
      "Epoch [76/600], Train Loss: 0.0381, Test Loss: 0.0519\n",
      "Epoch [77/600], Train Loss: 0.0457, Test Loss: 0.0557\n",
      "Epoch [78/600], Train Loss: 0.0443, Test Loss: 0.0638\n",
      "Epoch [79/600], Train Loss: 0.0388, Test Loss: 0.0634\n",
      "Epoch [80/600], Train Loss: 0.0277, Test Loss: 0.0415\n",
      "New best model saved with test loss: 0.0415\n",
      "Epoch [81/600], Train Loss: 0.0460, Test Loss: 0.1260\n",
      "Epoch [82/600], Train Loss: 0.0967, Test Loss: 0.1392\n",
      "Epoch [83/600], Train Loss: 0.0435, Test Loss: 0.0892\n",
      "Epoch [84/600], Train Loss: 0.0341, Test Loss: 0.1464\n",
      "Epoch [85/600], Train Loss: 0.0686, Test Loss: 0.0857\n",
      "Epoch [86/600], Train Loss: 0.0476, Test Loss: 0.0525\n",
      "Epoch [87/600], Train Loss: 0.0303, Test Loss: 0.0537\n",
      "Epoch [88/600], Train Loss: 0.0247, Test Loss: 0.0535\n",
      "Epoch [89/600], Train Loss: 0.0258, Test Loss: 0.0458\n",
      "Epoch [90/600], Train Loss: 0.0263, Test Loss: 0.0555\n",
      "Epoch [91/600], Train Loss: 0.0700, Test Loss: 0.0759\n",
      "Epoch [92/600], Train Loss: 0.0456, Test Loss: 0.0549\n",
      "Epoch [93/600], Train Loss: 0.0359, Test Loss: 0.0682\n",
      "Epoch [94/600], Train Loss: 0.0714, Test Loss: 0.0877\n",
      "Epoch [95/600], Train Loss: 0.0460, Test Loss: 0.0936\n",
      "Epoch [96/600], Train Loss: 0.0354, Test Loss: 0.1390\n",
      "Epoch [97/600], Train Loss: 0.0575, Test Loss: 0.0468\n",
      "Epoch [98/600], Train Loss: 0.0420, Test Loss: 0.0581\n",
      "Epoch [99/600], Train Loss: 0.0327, Test Loss: 0.0674\n",
      "Epoch [100/600], Train Loss: 0.0311, Test Loss: 0.0648\n",
      "Epoch [101/600], Train Loss: 0.0235, Test Loss: 0.0638\n",
      "Epoch [102/600], Train Loss: 0.0249, Test Loss: 0.0562\n",
      "Epoch [103/600], Train Loss: 0.0179, Test Loss: 0.0598\n",
      "Epoch [104/600], Train Loss: 0.0206, Test Loss: 0.0634\n",
      "Epoch [105/600], Train Loss: 0.0235, Test Loss: 0.0588\n",
      "Epoch [106/600], Train Loss: 0.0226, Test Loss: 0.0629\n",
      "Epoch [107/600], Train Loss: 0.0283, Test Loss: 0.0716\n",
      "Epoch [108/600], Train Loss: 0.0335, Test Loss: 0.0632\n",
      "Epoch [109/600], Train Loss: 0.0294, Test Loss: 0.0593\n",
      "Epoch [110/600], Train Loss: 0.0258, Test Loss: 0.0503\n",
      "Epoch [111/600], Train Loss: 0.0230, Test Loss: 0.0578\n",
      "Epoch [112/600], Train Loss: 0.0216, Test Loss: 0.0509\n",
      "Epoch [113/600], Train Loss: 0.0221, Test Loss: 0.0566\n",
      "Epoch [114/600], Train Loss: 0.0245, Test Loss: 0.0659\n",
      "Epoch [115/600], Train Loss: 0.0254, Test Loss: 0.0899\n",
      "Epoch [116/600], Train Loss: 0.0354, Test Loss: 0.0571\n",
      "Epoch [117/600], Train Loss: 0.0327, Test Loss: 0.0659\n",
      "Epoch [118/600], Train Loss: 0.0272, Test Loss: 0.0791\n",
      "Epoch [119/600], Train Loss: 0.0252, Test Loss: 0.0780\n",
      "Epoch [120/600], Train Loss: 0.0279, Test Loss: 0.0649\n",
      "Epoch [121/600], Train Loss: 0.0343, Test Loss: 0.0750\n",
      "Epoch [122/600], Train Loss: 0.0727, Test Loss: 0.0721\n",
      "Epoch [123/600], Train Loss: 0.0273, Test Loss: 0.0664\n",
      "Epoch [124/600], Train Loss: 0.0243, Test Loss: 0.0567\n",
      "Epoch [125/600], Train Loss: 0.0181, Test Loss: 0.0566\n",
      "Epoch [126/600], Train Loss: 0.0198, Test Loss: 0.0458\n",
      "Epoch [127/600], Train Loss: 0.0217, Test Loss: 0.0677\n",
      "Epoch [128/600], Train Loss: 0.0190, Test Loss: 0.0567\n",
      "Epoch [129/600], Train Loss: 0.0216, Test Loss: 0.0670\n",
      "Epoch [130/600], Train Loss: 0.0455, Test Loss: 0.1133\n",
      "Epoch [131/600], Train Loss: 0.0373, Test Loss: 0.0899\n",
      "Epoch [132/600], Train Loss: 0.0626, Test Loss: 0.1075\n",
      "Epoch [133/600], Train Loss: 0.1086, Test Loss: 0.2454\n",
      "Epoch [134/600], Train Loss: 0.1576, Test Loss: 0.1220\n",
      "Epoch [135/600], Train Loss: 0.0533, Test Loss: 0.0891\n",
      "Epoch [136/600], Train Loss: 0.0493, Test Loss: 0.1192\n",
      "Epoch [137/600], Train Loss: 0.1616, Test Loss: 0.1015\n",
      "Epoch [138/600], Train Loss: 0.0658, Test Loss: 0.1120\n",
      "Epoch [139/600], Train Loss: 0.0645, Test Loss: 0.0566\n",
      "Epoch [140/600], Train Loss: 0.0836, Test Loss: 0.0983\n",
      "Epoch [141/600], Train Loss: 0.0600, Test Loss: 0.1217\n",
      "Epoch [142/600], Train Loss: 0.0666, Test Loss: 0.0591\n",
      "Epoch [143/600], Train Loss: 0.0353, Test Loss: 0.0559\n",
      "Epoch [144/600], Train Loss: 0.0291, Test Loss: 0.0640\n",
      "Epoch [145/600], Train Loss: 0.0279, Test Loss: 0.0323\n",
      "New best model saved with test loss: 0.0323\n",
      "Epoch [146/600], Train Loss: 0.0288, Test Loss: 0.0593\n",
      "Epoch [147/600], Train Loss: 0.0277, Test Loss: 0.0480\n",
      "Epoch [148/600], Train Loss: 0.0233, Test Loss: 0.0337\n",
      "Epoch [149/600], Train Loss: 0.0282, Test Loss: 0.0554\n",
      "Epoch [150/600], Train Loss: 0.1086, Test Loss: 0.1034\n",
      "Epoch [151/600], Train Loss: 0.0813, Test Loss: 0.1452\n",
      "Epoch [152/600], Train Loss: 0.0481, Test Loss: 0.0581\n",
      "Epoch [153/600], Train Loss: 0.0364, Test Loss: 0.0400\n",
      "Epoch [154/600], Train Loss: 0.0234, Test Loss: 0.0503\n",
      "Epoch [155/600], Train Loss: 0.0221, Test Loss: 0.0481\n",
      "Epoch [156/600], Train Loss: 0.0208, Test Loss: 0.0417\n",
      "Epoch [157/600], Train Loss: 0.0281, Test Loss: 0.0558\n",
      "Epoch [158/600], Train Loss: 0.0229, Test Loss: 0.0453\n",
      "Epoch [159/600], Train Loss: 0.0253, Test Loss: 0.0479\n",
      "Epoch [160/600], Train Loss: 0.0386, Test Loss: 0.1121\n",
      "Epoch [161/600], Train Loss: 0.0435, Test Loss: 0.0475\n",
      "Epoch [162/600], Train Loss: 0.0187, Test Loss: 0.0576\n",
      "Epoch [163/600], Train Loss: 0.0273, Test Loss: 0.0759\n",
      "Epoch [164/600], Train Loss: 0.0236, Test Loss: 0.0412\n",
      "Epoch [165/600], Train Loss: 0.0179, Test Loss: 0.0542\n",
      "Epoch [166/600], Train Loss: 0.0195, Test Loss: 0.0765\n",
      "Epoch [167/600], Train Loss: 0.1253, Test Loss: 0.2375\n",
      "Epoch [168/600], Train Loss: 0.1158, Test Loss: 0.0685\n",
      "Epoch [169/600], Train Loss: 0.0599, Test Loss: 0.0425\n",
      "Epoch [170/600], Train Loss: 0.0931, Test Loss: 0.2024\n",
      "Epoch [171/600], Train Loss: 0.1073, Test Loss: 0.1191\n",
      "Epoch [172/600], Train Loss: 0.0651, Test Loss: 0.0686\n",
      "Epoch [173/600], Train Loss: 0.0423, Test Loss: 0.0479\n",
      "Epoch [174/600], Train Loss: 0.0844, Test Loss: 0.1631\n",
      "Epoch [175/600], Train Loss: 0.0669, Test Loss: 0.0874\n",
      "Epoch [176/600], Train Loss: 0.0315, Test Loss: 0.0366\n",
      "Epoch [177/600], Train Loss: 0.0293, Test Loss: 0.0654\n",
      "Epoch [178/600], Train Loss: 0.0549, Test Loss: 0.0595\n",
      "Epoch [179/600], Train Loss: 0.0400, Test Loss: 0.0496\n",
      "Epoch [180/600], Train Loss: 0.0216, Test Loss: 0.0329\n",
      "Epoch [181/600], Train Loss: 0.0192, Test Loss: 0.0361\n",
      "Epoch [182/600], Train Loss: 0.0408, Test Loss: 0.0548\n",
      "Epoch [183/600], Train Loss: 0.0250, Test Loss: 0.0430\n",
      "Epoch [184/600], Train Loss: 0.0227, Test Loss: 0.0398\n",
      "Epoch [185/600], Train Loss: 0.0203, Test Loss: 0.0304\n",
      "New best model saved with test loss: 0.0304\n",
      "Epoch [186/600], Train Loss: 0.0226, Test Loss: 0.0366\n",
      "Epoch [187/600], Train Loss: 0.0181, Test Loss: 0.0317\n",
      "Epoch [188/600], Train Loss: 0.0223, Test Loss: 0.0455\n",
      "Epoch [189/600], Train Loss: 0.0223, Test Loss: 0.0398\n",
      "Epoch [190/600], Train Loss: 0.0153, Test Loss: 0.0390\n",
      "Epoch [191/600], Train Loss: 0.0170, Test Loss: 0.0442\n",
      "Epoch [192/600], Train Loss: 0.0162, Test Loss: 0.0427\n",
      "Epoch [193/600], Train Loss: 0.0160, Test Loss: 0.0381\n",
      "Epoch [194/600], Train Loss: 0.0211, Test Loss: 0.0388\n",
      "Epoch [195/600], Train Loss: 0.0300, Test Loss: 0.0675\n",
      "Epoch [196/600], Train Loss: 0.1998, Test Loss: 0.2243\n",
      "Epoch [197/600], Train Loss: 0.0950, Test Loss: 0.0944\n",
      "Epoch [198/600], Train Loss: 0.0425, Test Loss: 0.0535\n",
      "Epoch [199/600], Train Loss: 0.0280, Test Loss: 0.0570\n",
      "Epoch [200/600], Train Loss: 0.0246, Test Loss: 0.0501\n",
      "Epoch [201/600], Train Loss: 0.0216, Test Loss: 0.0306\n",
      "Epoch [202/600], Train Loss: 0.0235, Test Loss: 0.0393\n",
      "Epoch [203/600], Train Loss: 0.0420, Test Loss: 0.0639\n",
      "Epoch [204/600], Train Loss: 0.0484, Test Loss: 0.0507\n",
      "Epoch [205/600], Train Loss: 0.0242, Test Loss: 0.0374\n",
      "Epoch [206/600], Train Loss: 0.0342, Test Loss: 0.1373\n",
      "Epoch [207/600], Train Loss: 0.0400, Test Loss: 0.0409\n",
      "Epoch [208/600], Train Loss: 0.0303, Test Loss: 0.0472\n",
      "Epoch [209/600], Train Loss: 0.0331, Test Loss: 0.0458\n",
      "Epoch [210/600], Train Loss: 0.0248, Test Loss: 0.0431\n",
      "Epoch [211/600], Train Loss: 0.0256, Test Loss: 0.0439\n",
      "Epoch [212/600], Train Loss: 0.0198, Test Loss: 0.0521\n",
      "Epoch [213/600], Train Loss: 0.0237, Test Loss: 0.0337\n",
      "Epoch [214/600], Train Loss: 0.0219, Test Loss: 0.0609\n",
      "Epoch [215/600], Train Loss: 0.0621, Test Loss: 0.1011\n",
      "Epoch [216/600], Train Loss: 0.1526, Test Loss: 0.1354\n",
      "Epoch [217/600], Train Loss: 0.0791, Test Loss: 0.0753\n",
      "Epoch [218/600], Train Loss: 0.0515, Test Loss: 0.0400\n",
      "Epoch [219/600], Train Loss: 0.0277, Test Loss: 0.3011\n",
      "Epoch [220/600], Train Loss: 0.0866, Test Loss: 0.0477\n",
      "Epoch [221/600], Train Loss: 0.0493, Test Loss: 0.0545\n",
      "Epoch [222/600], Train Loss: 0.0320, Test Loss: 0.0657\n",
      "Epoch [223/600], Train Loss: 0.0356, Test Loss: 0.0399\n",
      "Epoch [224/600], Train Loss: 0.0196, Test Loss: 0.0364\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import  ResNet18_Weights\n",
    "import copy\n",
    "from transformers import ConvNextConfig, ConvNextModel\n",
    "import math \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from utils import *\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]  # Add positional encoding\n",
    "        return x\n",
    "\n",
    "class Transformer2DPointsModel(nn.Module):\n",
    "    def __init__(self, input_dim,input_seq_len, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, dropout=0.1):\n",
    "        super(Transformer2DPointsModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear_in = nn.Linear(input_dim, d_model)  # Project input to d_model\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True),\n",
    "            num_encoder_layers\n",
    "        )\n",
    "        self.attention = AttentionLayer(d_model)  # Add attention layer\n",
    "        #self.linear_out = nn.Linear(d_model, output_dim)  # Map to the desired output size\n",
    "        self.linear_out = nn.Linear(d_model*input_seq_len, output_dim)  # Map to the desired output size\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.linear_in(src)  # Project input to d_model\n",
    "        src = self.positional_encoding(src)  # Add positional encoding\n",
    "        #memory = self.transformer_encoder(src, src_key_padding_mask=src_mask)  # Pass through transformer encoder\n",
    "                # memory shape: (batch_size, seq_len, d_model)\n",
    "        # Zero out padded tokens using src_mask\n",
    "        if src_mask is not None:\n",
    "            memory = memory * src_mask.unsqueeze(-1)\n",
    "            src_key_padding_mask = (src_mask == 0) # Convert 0/1 mask to boolean\n",
    "            memory = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
    "        else:\n",
    "            memory = self.transformer_encoder(src)\n",
    "            \n",
    "        # Apply attention mechanism\n",
    "        #aggregated = self.attention(memory, src_mask)  # Shape: (batch_size, d_model)\n",
    "\n",
    "        # Map to the desired output size\n",
    "        #output = self.linear_out(aggregated)  # Shape: (batch_size, output_dim)\n",
    "\n",
    "        memory = memory.view(memory.shape[0], -1)  # Shape: (batch_size, d_model, seq_len)\n",
    "        # Map to the desired output size\n",
    "        output = self.linear_out(memory)  # Shape: (batch_size, output_dim)\n",
    "        return output\n",
    "\n",
    "        \n",
    "# Training Setup with Best Model Saving\n",
    "def train_model(model, epochs=3, learning_rate=0.0001, optimizer= None, scheduler = None, train_loader=None, test_loader=None, norm_values=None):\n",
    "    #smoothness_loss_fn = SmoothnessLoss(weight=0.0)  # starting weight\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize variables for tracking best model\n",
    "    best_model_weights = None\n",
    "    best_test_loss = float('inf')\n",
    "    best_epoch=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        #smoothness_loss_fn.weight = time_varying_weight(epoch, 10, 20, max_weight=0.1)\n",
    "        for images, targets, init_data in train_loader:\n",
    "            images = images.to(device)\n",
    "            attention_mask = create_attention_mask(images).to(device)\n",
    "            targets = normalize_targets(targets,norm_values).to(device)              \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images,None)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            #loss2 = smoothness_loss_fn(outputs)\n",
    "            loss = loss #+ loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler is not None: scheduler.step()  # Update learning rate\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                denorm_outputs = denormalize_targets(outputs, norm_values)\n",
    "                denorm_targets = denormalize_targets(targets,norm_values)\n",
    "                denorm_loss = criterion(denorm_outputs, denorm_targets)\n",
    "                running_loss += denorm_loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss = running_loss/len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}\",end='')\n",
    "        writer.add_scalar('Loss/train', train_loss*1000, epoch+1)\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, targets, init_data in test_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device) \n",
    "                attention_mask = create_attention_mask(images).to(device)\n",
    "                outputs = model(images,None)\n",
    "                loss = criterion(denormalize_targets(outputs,norm_values), targets)\n",
    "                test_loss += loss.item()*images.size(0)\n",
    "        \n",
    "        mean_test_loss = test_loss/len(test_loader.dataset)\n",
    "        print(f\", Test Loss: {mean_test_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/test', mean_test_loss*1000, epoch+1)\n",
    "        # Save the model weights if this epoch has the lowest test loss so far\n",
    "        if mean_test_loss < best_test_loss:\n",
    "            best_test_loss = mean_test_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            print(f\"New best model saved with test loss: {best_test_loss:.4f}\")\n",
    "            best_epoch = epoch\n",
    "            writer.add_scalar('Loss/best_test', best_test_loss*1000, best_epoch+1)\n",
    "        writer.flush()\n",
    "    # Load the best model weights at the end of training\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"Loaded best model weights with test loss: {best_test_loss:.4f}, epoch: {best_epoch+1:3d}\")\n",
    "    else:\n",
    "        print(\"Warning: No best model weights found.\")\n",
    "    \n",
    "    print(\"Training Complete\")\n",
    "    return model\n",
    "\n",
    "writer = SummaryWriter('runs/experiment_name',flush_secs=5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_seq_len = 72\n",
    "input_dim = 4  # Each input is a sequence of 2Dx2 points (x, y)\n",
    "d_model = 512  # Embedding dimension\n",
    "nhead = 4  # Number of attention heads\n",
    "num_encoder_layers = 4  # Number of transformer encoder layers\n",
    "dim_feedforward = 512  # Feedforward network dimension\n",
    "output_dim = 280  # Each output is a 2D point (x, y)\n",
    "dropout = 0.0\n",
    "\n",
    "model = Transformer2DPointsModel(input_dim, input_seq_len, d_model, nhead, num_encoder_layers, dim_feedforward, output_dim, dropout).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")\n",
    "\n",
    "#dataset = ImagePairDataset(\"/mnt/hdd1/deformazione-2d-parametrica/\", transform=transform)\n",
    "dataset = PointsDataset(\"/mnt/hdd1/tesi_anna/tesi_anna/\", transform=None,padding=input_seq_len)\n",
    "test_split = 0.2 \n",
    "batch_size = 128\n",
    "# Split dataset into train and test sets\n",
    "test_size = int(test_split * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "norm_values=calculate_target_normalization(train_loader)\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 600\n",
    "warmup_steps = 100  # Number of warm-up steps\n",
    "total_steps = n_epochs * len(train_loader)  # Total training steps\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, total_steps=total_steps, pct_start=warmup_steps/total_steps)\n",
    "\n",
    "trained_model = train_model(model=model, epochs=n_epochs,learning_rate=learning_rate, optimizer=optimizer, scheduler=scheduler, train_loader=train_loader,test_loader=test_loader,norm_values=norm_values)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44f1584-53d7-4b60-accd-ae7faf745d4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrained_model\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ccdd39-f7ac-4071-9d14-d0b0d732797a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 54\u001b[0m\n\u001b[1;32m     47\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Example usage for a given sample index\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Assuming `init_coord_data`, `gt_data`, and `predicted_displacements` are NumPy arrays of shape (N, 2)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# For example:\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m images, targets, init_data \u001b[38;5;241m=\u001b[39m \u001b[43mtest_loader\u001b[49m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m78\u001b[39m]\n\u001b[1;32m     55\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m images \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_points_sequence(init_coord_data, gt_data, predicted_displacements):\n",
    "    \"\"\"\n",
    "    Plots the positions of points given the initial coordinates, ground truth displacements,\n",
    "    and predicted displacements. Also, plots lines connecting successive points for both sequences.\n",
    "\n",
    "    Args:\n",
    "        init_coord_data (numpy.ndarray): Initial positions of the points of shape (N, 2).\n",
    "        gt_data (numpy.ndarray): Ground truth displacement data of shape (N, 2).\n",
    "        predicted_displacements (numpy.ndarray): Predicted displacement data of shape (N, 2).\n",
    "    \"\"\"\n",
    "    # Calculate the final ground truth points\n",
    "    gt_points = init_coord_data + gt_data\n",
    "    \n",
    "    # Calculate the predicted points\n",
    "    predicted_points = init_coord_data + predicted_displacements\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot ground truth points (smaller 'x' markers)\n",
    "    plt.scatter(gt_points[:, 0], gt_points[:, 1], color='green', label='Ground Truth Points', marker='x', s=10)\n",
    "    \n",
    "    # Plot predicted points (smaller 'o' markers)\n",
    "    plt.scatter(predicted_points[:, 0], predicted_points[:, 1], color='red', label='Predicted Points', marker='o', s=10)\n",
    "    \n",
    "    # Connect the ground truth points to their successors\n",
    "    for i in range(len(gt_points) - 1):\n",
    "        plt.plot([gt_points[i, 0], gt_points[i + 1, 0]], [gt_points[i, 1], gt_points[i + 1, 1]], 'g-', alpha=0.5)\n",
    "    \n",
    "    # Connect the predicted points to their successors\n",
    "    for i in range(len(predicted_points) - 1):\n",
    "        plt.plot([predicted_points[i, 0], predicted_points[i + 1, 0]], [predicted_points[i, 1], predicted_points[i + 1, 1]], 'r-', alpha=0.5)\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('X Coordinate')\n",
    "    plt.ylabel('Y Coordinate')\n",
    "    plt.title('Point Displacements: Ground Truth vs. Predicted')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Show plot\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for a given sample index\n",
    "# Assuming `init_coord_data`, `gt_data`, and `predicted_displacements` are NumPy arrays of shape (N, 2)\n",
    "# For example:\n",
    "images, targets, init_data = test_loader.dataset[78]\n",
    "images = images.to(device)\n",
    "images = images[None, :, :]\n",
    "attention_mask = create_attention_mask(images).to(device)\n",
    "init_data = init_data.numpy().reshape(-1,2)\n",
    "targets = targets.numpy().reshape(-1,2)\n",
    "predicted_displacements = trained_model(images,None)\n",
    "predicted_displacements = denormalize_targets(predicted_displacements,norm_values)\n",
    "predicted_displacements = predicted_displacements.cpu().detach().numpy()\n",
    "predicted_displacements = predicted_displacements.reshape(-1,2)\n",
    "#import pdb; pdb.set_trace()\n",
    "# Call the function to plot the points for a sample\n",
    "plot_points_sequence(init_data, targets, predicted_displacements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c2669-c25d-4af2-8dea-ec18cebdccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_tensor(image_tensor):\n",
    "    \"\"\"\n",
    "    Takes a PyTorch tensor representing image data and plots the image.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): A tensor representing image data with shape \n",
    "                                     (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    # Check if the tensor is a single image or a batch\n",
    "    if image_tensor.ndimension() == 4:\n",
    "        # If batch size is greater than 1, let's plot the first image\n",
    "        image_tensor = image_tensor[0]  # Take the first image in the batch\n",
    "    \n",
    "    # Convert the tensor to a NumPy array and move channels to last dimension (for plotting)\n",
    "    image_array = image_tensor.numpy().transpose((1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n",
    "    #image_array[:, :, 1] = 1\n",
    "    # If the image is grayscale (single channel), it will have only one channel\n",
    "    if image_array.shape[2] == 1:\n",
    "        image_array = image_array[:, :, 0]  # Convert to 2D for grayscale images\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(image_array, cmap='gray' if image_array.ndim == 2 else None)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "plot_image_tensor(images[:,:,:,:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9831b-c652-4fa9-aa30-42f7761b3648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6540c-dabf-4ba1-8fd1-8f90123a4437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59667800-296c-4c82-b5b4-b862b54cb38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f820c5-d50a-4717-9156-dd2713dfeca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c593dc-8b84-420c-807c-00dcc6475145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecb627-50ba-4902-98d9-8ba77833c464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c59825-2bea-4bce-bafd-daa030968ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105650f6-4a22-4613-8195-45a38920e9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f373f-1019-41a2-b044-6127d0d91f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca485dca-55f0-446e-aa0e-e67e73e09f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13ee35a-9256-4ad3-b3cf-a3b2b7c46af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
